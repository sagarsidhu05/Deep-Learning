{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189N5sesV1No"
      },
      "source": [
        "**Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**\n",
        "#### **Answer**:\n",
        "**Filters (Kernels):**  \n",
        "- Small learnable matrices (e.g., 3×3 or 5×5) that slide over the input image.  \n",
        "- Each filter detects a specific pattern such as edges, textures, or shapes.  \n",
        "- During training, the network learns the optimal filter values to capture useful features.\n",
        "\n",
        "**Feature Maps (Activation Maps):**  \n",
        "- The output generated when a filter is convolved with the input.  \n",
        "- Highlight *where* a particular feature (edge, corner, texture) appears in the image.  \n",
        "- Stacking multiple feature maps from different filters forms the representation for the next layer.\n",
        "\n",
        "> **In short:**  \n",
        "> **Filters** = *what to look for*  \n",
        "> **Feature Maps** = *where it is found*\n",
        "\n",
        "\n",
        "#**Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?**\n",
        "#### **Answer**:\n",
        "#### **Padding**\n",
        "- **Definition:** Adding extra pixels (usually zeros) around the input image’s border before applying the convolution.\n",
        "- **Purpose:**\n",
        "  - Preserve spatial size after convolution.\n",
        "  - Avoid losing edge information.\n",
        "- **Common Types:**\n",
        "  - **Valid (no padding):** No extra pixels. Output shrinks.\n",
        "  - **Same (zero padding):** Pads so output size ≈ input size when stride = 1.\n",
        "\n",
        "#### **Stride**\n",
        "- **Definition:** The number of pixels the filter moves at each step horizontally and vertically.\n",
        "- **Effects:**\n",
        "  - Larger stride → skips more pixels → smaller output.\n",
        "  - Stride of 1 → slides filter one pixel at a time → larger output.\n",
        "\n",
        "\n",
        "###  Output Dimension Formula\n",
        "$$\n",
        "\\text{Output Height} = \\left\\lfloor \\frac{H - K + 2P}{S} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Output Width} = \\left\\lfloor \\frac{W - K + 2P}{S} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $H, W$ = input height & width  \n",
        "- $K$ = kernel size  \n",
        "- $P$ = padding  \n",
        "- $S$ = stride  \n",
        "\n",
        "\n",
        "\n",
        "###  Quick Examples\n",
        "1. **No padding, stride 1**  \n",
        "   - Input: 32×32, Kernel: 3×3  \n",
        "   - Output: (32 − 3 + 0)/1 + 1 = **30×30**\n",
        "\n",
        "2. **Same padding, stride 1**  \n",
        "   - Input: 32×32, Kernel: 3×3, Padding: 1  \n",
        "   - Output: (32 − 3 + 2)/1 + 1 = **32×32**\n",
        "\n",
        "3. **Padding 1, stride 2**  \n",
        "   - Input: 32×32, Kernel: 3×3  \n",
        "   - Output: ⌊(32 − 3 + 2)/2⌋ + 1 = **16×16**\n",
        "   \n",
        "\n",
        "#**Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?**\n",
        "#### **Answer**:\n",
        "**Definition:**  \n",
        "The **receptive field** is the region of the input image that influences a neuron’s activation.  \n",
        "Example: a $3 \\times 3$ kernel gives each neuron a $3 \\times 3$ receptive field.\n",
        "\n",
        "**Growth in Deep Networks:**  \n",
        "With stacked layers, the effective receptive field enlarges:\n",
        "\n",
        "$$\n",
        "R_l = R_{l-1} + (k_l - 1)\\!\\times\\!\\prod_{i=1}^{l-1} s_i\n",
        "$$\n",
        "\n",
        "where $k_l$ = kernel size, $s_i$ = stride, $R_1 = k_1$.\n",
        "\n",
        "**Importance:**  \n",
        "- Captures broader context for detecting large objects.  \n",
        "- Enables higher layers to recognize full shapes, not just edges.  \n",
        "- Guides network design to ensure the final layers “see” enough of the input.\n",
        "\n",
        "\n",
        "#**Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.**\n",
        "#### **Answer**:\n",
        "**1️ Filter (Kernel) Size**\n",
        "- Parameters in a conv layer depend on:\n",
        "  $$\n",
        "  \\text{Params} = (K_h \\times K_w \\times C_{in}) \\times C_{out} + C_{out}\n",
        "  $$\n",
        "  where  \n",
        "  - $K_h, K_w$ = filter height & width  \n",
        "  - $C_{in}$ = input channels  \n",
        "  - $C_{out}$ = number of filters (output channels)  \n",
        "- **Impact:** Larger filters ($K_h,K_w$) ⇒ **more parameters** because each filter contains more weights.\n",
        "\n",
        "**2️ Stride**\n",
        "- **Stride does *not* change the number of trainable parameters**, because filter dimensions and counts stay the same.\n",
        "- It only changes the **output feature-map size**, thus affecting:\n",
        "  - Memory usage\n",
        "  - Computational cost (fewer output locations when stride is large).\n",
        "\n",
        "\n",
        "**Summary Table**\n",
        "\n",
        "| Hyperparameter | Parameters | Output Size |\n",
        "|----------------|-----------|------------|\n",
        "| **Filter size ↑** | ↑ (more weights) | Slightly smaller (unless padded) |\n",
        "| **Stride ↑**    | No change | ↓ (fewer activations) |\n",
        "\n",
        "\n",
        "#**Question 5: Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.**\n",
        "#### **Answer**:\n",
        "| Feature            | **LeNet-5 (1998)** | **AlexNet (2012)**          | **VGG-16/19 (2014)** |\n",
        "|--------------------|--------------------|-----------------------------|----------------------|\n",
        "| **Depth**         | ~5 learnable layers (2 conv + 3 FC) | 8 learnable layers (5 conv + 3 FC) | 16 or 19 learnable layers (13/16 conv + 3 FC) |\n",
        "| **Filter Sizes**   | Mostly $5 \\times 5$ | First layer $11 \\times 11$, others $5 \\times 5$ and $3 \\times 3$ | Uniform $3 \\times 3$ filters throughout |\n",
        "| **Input Size**     | $32 \\times 32$ grayscale | $224 \\times 224$ RGB | $224 \\times 224$ RGB |\n",
        "| **Parameters**     | ~60K               | ~60M                        | ~138M (VGG-16)       |\n",
        "| **Performance / Impact** | Pioneered CNNs for digit recognition (MNIST) | Won ImageNet 2012, introduced ReLU & dropout, GPU training | High accuracy on ImageNet; showed that deep networks with small filters work well |\n",
        "| **Key Innovations** | Early CNN with subsampling | Overlapping max-pooling, data augmentation | Deep, uniform architecture for scalable depth |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CtepnJjaXJ6"
      },
      "source": [
        "#**Question 6: Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "844/844 - 8s - 9ms/step - accuracy: 0.9430 - loss: 0.1891 - val_accuracy: 0.9807 - val_loss: 0.0650\n",
            "Epoch 2/5\n",
            "844/844 - 6s - 8ms/step - accuracy: 0.9832 - loss: 0.0550 - val_accuracy: 0.9888 - val_loss: 0.0407\n",
            "Epoch 3/5\n",
            "844/844 - 6s - 7ms/step - accuracy: 0.9880 - loss: 0.0389 - val_accuracy: 0.9900 - val_loss: 0.0379\n",
            "Epoch 4/5\n",
            "844/844 - 6s - 8ms/step - accuracy: 0.9908 - loss: 0.0304 - val_accuracy: 0.9883 - val_loss: 0.0392\n",
            "Epoch 5/5\n",
            "844/844 - 6s - 7ms/step - accuracy: 0.9922 - loss: 0.0234 - val_accuracy: 0.9880 - val_loss: 0.0415\n",
            "Test accuracy: 0.9869, Test loss: 0.0400\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test  = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=2)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.4f}, Test loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maW1cY5Pa0dh"
      },
      "source": [
        "#**Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "kSt8tG1tb2he",
        "outputId": "62ccad93-2e61-4b69-8c15-a07da9dd1b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1us/step\n",
            "Epoch 1/10\n",
            "704/704 - 31s - 44ms/step - accuracy: 0.4232 - loss: 1.5810 - val_accuracy: 0.5918 - val_loss: 1.1479\n",
            "Epoch 2/10\n",
            "704/704 - 30s - 42ms/step - accuracy: 0.5924 - loss: 1.1397 - val_accuracy: 0.6560 - val_loss: 0.9747\n",
            "Epoch 3/10\n",
            "704/704 - 30s - 42ms/step - accuracy: 0.6592 - loss: 0.9659 - val_accuracy: 0.7260 - val_loss: 0.7903\n",
            "Epoch 4/10\n",
            "704/704 - 30s - 43ms/step - accuracy: 0.6970 - loss: 0.8561 - val_accuracy: 0.7432 - val_loss: 0.7372\n",
            "Epoch 5/10\n",
            "704/704 - 30s - 42ms/step - accuracy: 0.7274 - loss: 0.7748 - val_accuracy: 0.7320 - val_loss: 0.7806\n",
            "Epoch 6/10\n",
            "704/704 - 29s - 42ms/step - accuracy: 0.7472 - loss: 0.7199 - val_accuracy: 0.7644 - val_loss: 0.6926\n",
            "Epoch 7/10\n",
            "704/704 - 29s - 42ms/step - accuracy: 0.7661 - loss: 0.6670 - val_accuracy: 0.7556 - val_loss: 0.7171\n",
            "Epoch 8/10\n",
            "704/704 - 29s - 41ms/step - accuracy: 0.7779 - loss: 0.6268 - val_accuracy: 0.7804 - val_loss: 0.6618\n",
            "Epoch 9/10\n",
            "704/704 - 30s - 42ms/step - accuracy: 0.7921 - loss: 0.5867 - val_accuracy: 0.7798 - val_loss: 0.6730\n",
            "Epoch 10/10\n",
            "704/704 - 29s - 41ms/step - accuracy: 0.8047 - loss: 0.5507 - val_accuracy: 0.7858 - val_loss: 0.6514\n",
            "Test accuracy: 0.7785, Test loss: 0.6632\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\") / 255.0\n",
        "y_train = y_train.flatten()\n",
        "y_test  = y_test.flatten()\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test,verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.4f}, Test loss: {test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6QBaWIoa8EC"
      },
      "source": [
        "#**Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "tyqkxeVhcjBR",
        "outputId": "beca3206-c167-466f-9bbf-6e9a8c6850e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n",
            "100.0%\n",
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.1898\n",
            "Epoch 2, Loss: 0.0805\n",
            "Epoch 3, Loss: 0.0620\n",
            "Epoch 4, Loss: 0.0506\n",
            "Epoch 5, Loss: 0.0426\n",
            "Test Accuracy: 99.06%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# MNIST Data loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# CNN Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNVu4xoFbEsM"
      },
      "source": [
        "**Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 294 images belonging to 2 classes.\n",
            "Found 97 images belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.4626 - loss: 0.7966"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Abhishek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.4320 - loss: 0.7738 - val_accuracy: 0.4330 - val_loss: 0.6906\n",
            "Epoch 2/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.6259 - loss: 0.6805 - val_accuracy: 0.6082 - val_loss: 0.6410\n",
            "Epoch 3/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - accuracy: 0.6973 - loss: 0.6042 - val_accuracy: 0.5464 - val_loss: 0.7456\n",
            "Epoch 4/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 179ms/step - accuracy: 0.6599 - loss: 0.6170 - val_accuracy: 0.6598 - val_loss: 0.6400\n",
            "Epoch 5/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.6327 - loss: 0.6317 - val_accuracy: 0.6289 - val_loss: 0.6270\n",
            "Epoch 6/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - accuracy: 0.6939 - loss: 0.5937 - val_accuracy: 0.5876 - val_loss: 0.7027\n",
            "Epoch 7/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.6905 - loss: 0.6028 - val_accuracy: 0.4433 - val_loss: 0.8639\n",
            "Epoch 8/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.5612 - loss: 0.6917 - val_accuracy: 0.6804 - val_loss: 0.6249\n",
            "Epoch 9/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 172ms/step - accuracy: 0.6939 - loss: 0.6173 - val_accuracy: 0.6495 - val_loss: 0.6223\n",
            "Epoch 10/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.6871 - loss: 0.6040 - val_accuracy: 0.7010 - val_loss: 0.5791\n",
            "Epoch 11/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 182ms/step - accuracy: 0.7143 - loss: 0.5664 - val_accuracy: 0.7113 - val_loss: 0.5486\n",
            "Epoch 12/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 190ms/step - accuracy: 0.7313 - loss: 0.6077 - val_accuracy: 0.6701 - val_loss: 0.6320\n",
            "Epoch 13/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.6837 - loss: 0.6101 - val_accuracy: 0.6907 - val_loss: 0.6184\n",
            "Epoch 14/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 187ms/step - accuracy: 0.6939 - loss: 0.5581 - val_accuracy: 0.7216 - val_loss: 0.5753\n",
            "Epoch 15/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - accuracy: 0.7245 - loss: 0.5594 - val_accuracy: 0.7113 - val_loss: 0.5360\n",
            "Epoch 16/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 204ms/step - accuracy: 0.7313 - loss: 0.5701 - val_accuracy: 0.7526 - val_loss: 0.4827\n",
            "Epoch 17/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - accuracy: 0.7925 - loss: 0.5176 - val_accuracy: 0.7320 - val_loss: 0.4770\n",
            "Epoch 18/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.7381 - loss: 0.5254 - val_accuracy: 0.7010 - val_loss: 0.6115\n",
            "Epoch 19/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - accuracy: 0.7483 - loss: 0.5500 - val_accuracy: 0.7423 - val_loss: 0.4607\n",
            "Epoch 20/20\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - accuracy: 0.7347 - loss: 0.5250 - val_accuracy: 0.7423 - val_loss: 0.4886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Paths to your dataset\n",
        "train_dir = 'datasets/train'\n",
        "val_dir = 'datasets/validation'\n",
        "\n",
        "# ImageDataGenerator for preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,          # Normalize pixel values to [0,1]\n",
        "    rotation_range=20,       # Random rotations\n",
        "    width_shift_range=0.2,   # Horizontal shift\n",
        "    height_shift_range=0.2,  # Vertical shift\n",
        "    shear_range=0.2,         # Shear transformation\n",
        "    zoom_range=0.2,          # Random zoom\n",
        "    horizontal_flip=True,    # Flip horizontally\n",
        "    fill_mode='nearest'      # Fill pixels after transformation\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255           # Only normalize for validation\n",
        ")\n",
        "\n",
        "# Load images from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(128, 128),  # Resize all images\n",
        "    batch_size=32,\n",
        "    class_mode='categorical' # For multi-class classification\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build a simple CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    \n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    \n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(train_generator.class_indices), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('custom_cnn_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5HF4Ef9bH4n"
      },
      "source": [
        "**Question 10: You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data: sources and preparation**\n",
        "\n",
        "Datasets\n",
        "\n",
        "Use a labeled chest X-ray dataset (example: “Chest X-Ray Images (Pneumonia)” often available on Kaggle / institutional dataset). If using internal clinical data, follow privacy rules (de-identify images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Abhishek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/chest-xray-pneumonia?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.29G/2.29G [02:57<00:00, 13.9MB/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\Abhishek\\.cache\\kagglehub\\datasets\\paultimothymooney\\chest-xray-pneumonia\\versions\\2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Data Preparation**\n",
        "\n",
        "Theory:\n",
        "Medical images often vary in size, quality, and orientation. Preprocessing is crucial to normalize images and augment the dataset to improve model generalization. Typical steps include resizing, normalization, and data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0koRTx2qavA6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'chest_xray/train',\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    'chest_xray/val',\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Building the CNN Model**\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "A CNN (Convolutional Neural Network) is ideal for image classification because it automatically extracts spatial features using convolutional layers.\n",
        "\n",
        "**Common layers:**\n",
        "- Conv2D: Extracts features using filters.\n",
        "- MaxPooling2D: Reduces spatial dimensions.\n",
        "- Flatten & Dense: Converts features into a vector for classification.\n",
        "- Dropout: Prevents overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Abhishek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    \n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    \n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')   # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Model Training**\n",
        "**Theory:**\n",
        "\n",
        "- Binary crossentropy is used for 2-class problems.\n",
        "- Adam optimizer is commonly used for faster convergence.\n",
        "- Dropout reduces overfitting, especially in small medical datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Abhishek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 459ms/step - accuracy: 0.7857 - loss: 0.4848 - val_accuracy: 0.7500 - val_loss: 0.4606\n",
            "Epoch 2/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 416ms/step - accuracy: 0.8585 - loss: 0.3150 - val_accuracy: 0.6250 - val_loss: 0.9034\n",
            "Epoch 3/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 425ms/step - accuracy: 0.8921 - loss: 0.2538 - val_accuracy: 0.6875 - val_loss: 0.7217\n",
            "Epoch 4/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 428ms/step - accuracy: 0.9041 - loss: 0.2292 - val_accuracy: 0.6875 - val_loss: 0.9618\n",
            "Epoch 5/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 465ms/step - accuracy: 0.9147 - loss: 0.2171 - val_accuracy: 0.7500 - val_loss: 0.4716\n",
            "Epoch 6/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 501ms/step - accuracy: 0.9166 - loss: 0.2086 - val_accuracy: 0.8125 - val_loss: 0.2829\n",
            "Epoch 7/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 423ms/step - accuracy: 0.9247 - loss: 0.1883 - val_accuracy: 0.7500 - val_loss: 0.4603\n",
            "Epoch 8/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 422ms/step - accuracy: 0.9329 - loss: 0.1809 - val_accuracy: 0.8125 - val_loss: 0.2766\n",
            "Epoch 9/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 427ms/step - accuracy: 0.9348 - loss: 0.1661 - val_accuracy: 0.9375 - val_loss: 0.1907\n",
            "Epoch 10/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 431ms/step - accuracy: 0.9379 - loss: 0.1683 - val_accuracy: 0.6875 - val_loss: 0.6879\n",
            "Epoch 11/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 418ms/step - accuracy: 0.9434 - loss: 0.1548 - val_accuracy: 0.6875 - val_loss: 0.4469\n",
            "Epoch 12/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 416ms/step - accuracy: 0.9411 - loss: 0.1631 - val_accuracy: 0.8125 - val_loss: 0.3042\n",
            "Epoch 13/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 418ms/step - accuracy: 0.9461 - loss: 0.1427 - val_accuracy: 0.8125 - val_loss: 0.2568\n",
            "Epoch 14/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 425ms/step - accuracy: 0.9515 - loss: 0.1389 - val_accuracy: 0.6875 - val_loss: 0.3375\n",
            "Epoch 15/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 413ms/step - accuracy: 0.9471 - loss: 0.1414 - val_accuracy: 0.8750 - val_loss: 0.2259\n",
            "Epoch 16/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 412ms/step - accuracy: 0.9519 - loss: 0.1354 - val_accuracy: 0.6250 - val_loss: 0.9172\n",
            "Epoch 17/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 413ms/step - accuracy: 0.9567 - loss: 0.1336 - val_accuracy: 0.7500 - val_loss: 0.4792\n",
            "Epoch 18/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 421ms/step - accuracy: 0.9511 - loss: 0.1315 - val_accuracy: 1.0000 - val_loss: 0.1645\n",
            "Epoch 19/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 414ms/step - accuracy: 0.9523 - loss: 0.1289 - val_accuracy: 0.6250 - val_loss: 0.5646\n",
            "Epoch 20/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 427ms/step - accuracy: 0.9546 - loss: 0.1222 - val_accuracy: 0.9375 - val_loss: 0.1717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('chest_xray_cnn.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Model Evaluation**\n",
        "- Plot accuracy and loss curves for visual inspection.\n",
        "- Check for misclassifications and confusion matrix for deeper insight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 624 images belonging to 2 classes.\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 210ms/step - accuracy: 0.8397 - loss: 0.4638\n",
            "Test Accuracy: 83.97%\n"
          ]
        }
      ],
      "source": [
        "test_generator = val_datagen.flow_from_directory(\n",
        "    'chest_xray/test',\n",
        "    target_size=(128,128),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Deploying as a Web App using Streamlit**\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "Streamlit is a Python framework for creating interactive web apps easily. Users can upload X-ray images, and the app displays the model’s prediction.\n",
        "\n",
        "Streamlit App Code (app.py):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load trained model\n",
        "model = load_model('chest_xray_cnn.h5')\n",
        "\n",
        "st.title(\"Chest X-ray Pneumonia Classifier\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a chest X-ray image\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    img = image.load_img(uploaded_file, target_size=(128,128))\n",
        "    img_array = image.img_to_array(img)/255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    prediction = model.predict(img_array)[0][0]\n",
        "    \n",
        "    if prediction < 0.5:\n",
        "        st.success(\"Prediction: Normal\")\n",
        "    else:\n",
        "        st.error(\"Prediction: Pneumonia\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
